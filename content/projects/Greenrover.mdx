---
title: Green Rover - Autonomous Trash Detection
description: Green Rover is an autonomous ground robot designed to detect and locate plastic waste in real time while navigating complex environments without human intervention. By combining on device computer vision with responsive motion control, the system enables stable, continuous operation in real world cleanup scenarios.
teaser: "Autonomous AI object detection on ESP32 and Raspberry Pi 4"
date: "2025-11-01"
published: true
image: /Gambar/rova.png
tags:
  - "IoT"
  - "ESP32"
  - "Raspberry Pi 4"
  - "Computer Vision"
  - "Python"
  - "Roboflow"
  - "3D Printing"
status: "Completed"
platform: "IoT & Robotics"
highlights:
  - "14.5 FPS on device"
  - "Dual control architecture"
---

# The Mission

Plastic pollution is a crisis. Manual cleanup is slow. **Green Rover** is the solution.

I engineered this robot, it is a fully autonomous agent capable of patrolling environments, identifying waste with good perception, and navigating complex terrain without intervention.

<YouTube id= "Gm7ZFIPPxAQ" />

---

## Real-Time Detection in Action

<div className="my-8 flex justify-center">
  <div className="max-w-lg w-full"> 
    <Image 
      src="/Gambar/detect.png" 
      alt="Real-time plastic bottle detection with bounding boxes" 
      width={1280} 
      height={720} 
      className="rounded-xl border border-zinc-800 shadow-xl"
    />
  </div>
</div>

During operation, Green Rover performs **continuous on device inference**, detecting plastic waste in real time while navigating autonomously.  
Each detection includes **class labels and spatial positioning**, enabling stable tracking even under uneven lighting and cluttered backgrounds.

## The Architecture

<div className="my-8">
  <Image 
    src="/Gambar/ROVER.png"
    alt="Final Assembly of the Acrylic Thermal Chamber" 
    width={1920} 
    height={1080} 
    className="rounded-xl border border-zinc-800 shadow-2xl" 
  />
</div>

The core innovation of Green Rover is its **Hybrid Dual Controller System**, a "Brain and Muscle" architecture designed to overcome the hardware limitations of standard IoT devices.

### The Brain: Raspberry Pi 4
* **Role:** High Level Intelligence.
* **Function:** Runs the YOLOv8 Neural Network, processes voice commands, and hosts the web dashboard.
* **Why:** It delegates all physical exertion to the ESP32, ensuring the AI never stutters.

### The Muscle: ESP32
* **Role:** Real Time Reflexes.
* **Function:** Generates precise PWM signals for motors, controls the pan tilt servo, and reads ultrasonic sensors in microseconds.
* **Impact:** This split architecture boosted system performance by **300%**, jumping from 4.5 FPS to **14.5 FPS**.

---

## Hardware & Components

Every component was selected for maximum efficiency and integration.

| Component | Specification | Role |
| :--- | :--- | :--- |
| **Main Computer** | Raspberry Pi 4 Model B | AI Inference & Web Server |
| **Microcontroller** | ESP32 | Motor & Sensor Control |
| **Vision** | Raspberry Pi Camera Module | Visual Input for YOLOv8 |
| **Actuators** | DC Motors + L298N Driver | Propulsion & Steering |
| **Mechanics** | SG90 Servo | Pan Tilt Camera Mechanism |
| **Sensors** | 4x HC-SR04 Ultrasonic | Obstacle Avoidance (Front/Back/Sides) |
| **Power** | 3x18650 Li-ion(3.7v) + Power Bank | Independent power for motors vs. logic |

---

## ðŸ§  AI Engineering with Roboflow

Data is the lifeblood of AI. I didn't just use a pre made model, I built a custom visual engine tailored for the Indonesian environment.

* **Custom Dataset:** I curated and labeled **950 images** using **Roboflow**, specifically targeting local plastic waste.
* **Specific Classes:** The model is trained to distinguish between generic plastic and specific local brands like **Aqua, Cleo, and Le Minerale** bottles.
* **Training:** Leveraging **YOLOv8 Nano**, I optimized the model for edge deployment, achieving a balance of speed and accuracy that allows for real-time tracking in dynamic lighting.

---

## Observed Performance Metrics

| Metric | Result |
|------|-------|
| Inference Speed | ~14.5 FPS (on-device) |
| Detection Range | ~2â€“2.5 meters (indoor testing) |
| Runtime Stability | Continuous operation without frame drops |
| Power Separation | Stable inference under motor load |

All measurements were observed during real-world testing rather than simulated environments.

---

## Challenges

Building a custom robot from scratch came with significant engineering challenges:

1.  **The Latency Bottleneck:** Initially, running both the AI and motor control on the Raspberry Pi caused the system to freeze. I solved this by decoupling the systems into the "Brain & Muscle" architecture.
2.  **Lighting Conditions:** The vision system struggled in dim indoor lighting. I had to fine tune the camera's ISO gain and implement a dynamic confidence threshold in the code to prevent false positives.
3.  **Sensor Interference:** The ultrasonic sensors would sometimes interfere with each other. I wrote a custom timing algorithm in C++ for the ESP32 to trigger them sequentially rather than simultaneously.

---

## My Role & Ownership

This project was **designed, built, and integrated end to end** by me, covering:

* Dataset collection and annotation
* YOLOv8 model training and optimization
* ESP32 firmware (motor control, sensor timing)
* Raspberry Pi AI pipeline and system orchestration
* Mechanical assembly and electronics integration

---

## Future Improvements

While the system is currently functional, I plan to push the boundaries further:

* **Add 2 DOF Hand:** I want to add a robotic hand so it can also pick up trash.
* **Adaptive Navigation:** Upgrading the obstacle avoidance logic from simple "stop and turn" to pathfinding algorithms like.
* **Battery Management:** Designing a custom PCB for power distribution to replace the bulky power bank setup.
